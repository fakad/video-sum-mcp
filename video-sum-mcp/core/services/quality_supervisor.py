"""
æ™ºèƒ½è´¨é‡ç›‘ç£æ¨¡å—

ç”¨äºç›‘ç£æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­çš„æ–‡æ¡£è´¨é‡ï¼Œé˜²æ­¢ç”Ÿæˆç›¸åŒæˆ–é€šç”¨æ¨¡æ¿æ–‡æ¡£ï¼Œ
é›†æˆLLMè¾…åŠ©è¯Šæ–­ï¼Œç¡®ä¿æ¯ä¸ªè§†é¢‘é“¾æ¥éƒ½èƒ½ç”ŸæˆçœŸå®ã€æœ‰ä»·å€¼çš„å†…å®¹ã€‚
"""

import os
import json
import hashlib
import time
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import logging
from dataclasses import dataclass, asdict
from collections import defaultdict

logger = logging.getLogger(__name__)

@dataclass
class DocumentQualityMetrics:
    """æ–‡æ¡£è´¨é‡æŒ‡æ ‡"""
    url: str
    title: str
    content_length: int
    unique_keywords: int
    content_hash: str
    platform: str
    author: str
    timestamp: float
    quality_score: float
    issues: List[str]
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

@dataclass
class BatchQualityReport:
    """æ‰¹é‡å¤„ç†è´¨é‡æŠ¥å‘Š"""
    batch_id: str
    total_documents: int
    unique_documents: int
    duplicate_documents: int
    template_documents: int
    quality_issues: List[str]
    platform_stats: Dict[str, Dict[str, Any]]
    recommendations: List[str]
    timestamp: float
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

class QualitySupervisor:
    """æ™ºèƒ½è´¨é‡ç›‘ç£å™¨"""
    
    def __init__(self, 
                 enable_llm_assistance: bool = True,
                 similarity_threshold: float = 0.85,
                 min_quality_score: float = 0.6):
        """
        åˆå§‹åŒ–è´¨é‡ç›‘ç£å™¨
        
        å‚æ•°:
            enable_llm_assistance: æ˜¯å¦å¯ç”¨LLMè¾…åŠ©è¯Šæ–­
            similarity_threshold: æ–‡æ¡£ç›¸ä¼¼åº¦é˜ˆå€¼
            min_quality_score: æœ€ä½è´¨é‡åˆ†æ•°
        """
        self.enable_llm_assistance = enable_llm_assistance
        self.similarity_threshold = similarity_threshold
        self.min_quality_score = min_quality_score
        
        # ç›‘ç£æ•°æ®å­˜å‚¨
        self.batch_history: List[BatchQualityReport] = []
        self.document_metrics: List[DocumentQualityMetrics] = []
        self.content_hashes: Dict[str, List[str]] = defaultdict(list)  # hash -> urls
        
        # é€šç”¨æ¨¡æ¿æ£€æµ‹å…³é”®è¯
        self.template_keywords = {
            "ç»¼åˆçŸ¥è¯†", "æœªçŸ¥æ ‡é¢˜", "æœªçŸ¥ä½œè€…", "è§†é¢‘", "å†…å®¹åˆ†æ",
            "çŸ¥è¯†å›¾è°±", "æ ¸å¿ƒæ¦‚å¿µ", "å®ç”¨æŠ€å·§", "æ·±åº¦è§è§£"
        }
        
        # å¹³å°ç‰¹å®šè´¨é‡æ ‡å‡†
        self.platform_standards = {
            "douyin": {
                "min_title_length": 5,
                "min_content_length": 200,
                "required_fields": ["title", "author", "description"],
                "forbidden_titles": ["è§†é¢‘", "æŠ–éŸ³è§†é¢‘", "æœªçŸ¥æ ‡é¢˜"]
            },
            "xiaohongshu": {
                "min_title_length": 3,
                "min_content_length": 150,
                "required_fields": ["title", "author"],
                "forbidden_titles": ["å°çº¢ä¹¦", "ç¬”è®°", "æœªçŸ¥æ ‡é¢˜"]
            },
            "zhihu": {
                "min_title_length": 8,
                "min_content_length": 300,
                "required_fields": ["title", "author"],
                "forbidden_titles": ["çŸ¥ä¹", "å›ç­”", "æœªçŸ¥æ ‡é¢˜"]
            },
            "bilibili": {
                "min_title_length": 5,
                "min_content_length": 200,
                "required_fields": ["title", "author"],
                "forbidden_titles": ["Bç«™", "è§†é¢‘", "æœªçŸ¥æ ‡é¢˜"]
            }
        }
        
        logger.info(f"åˆå§‹åŒ–è´¨é‡ç›‘ç£å™¨: LLMè¾…åŠ©={'å¯ç”¨' if enable_llm_assistance else 'ç¦ç”¨'}")
    
    def analyze_document_quality(self, 
                                url: str, 
                                content_data: Dict[str, Any], 
                                generated_content: str) -> DocumentQualityMetrics:
        """
        åˆ†æå•ä¸ªæ–‡æ¡£çš„è´¨é‡
        
        å‚æ•°:
            url: è§†é¢‘URL
            content_data: æå–çš„å†…å®¹æ•°æ®
            generated_content: ç”Ÿæˆçš„æ–‡æ¡£å†…å®¹
            
        è¿”å›:
            æ–‡æ¡£è´¨é‡æŒ‡æ ‡
        """
        metadata = content_data.get('metadata', {})
        title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
        author = metadata.get('author', 'æœªçŸ¥ä½œè€…')
        platform = content_data.get('platform', 'unknown')
        
        # è®¡ç®—å†…å®¹å“ˆå¸Œ
        content_hash = hashlib.md5(generated_content.encode('utf-8')).hexdigest()
        
        # åˆ†æè´¨é‡æŒ‡æ ‡
        issues = []
        quality_score = 1.0
        
        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºé€šç”¨æ¨¡æ¿
        template_score = self._check_template_content(title, generated_content)
        if template_score > 0.7:
            issues.append(f"ç–‘ä¼¼é€šç”¨æ¨¡æ¿ (ç›¸ä¼¼åº¦: {template_score:.2f})")
            quality_score -= 0.4
        
        # 2. æ£€æŸ¥å¹³å°ç‰¹å®šæ ‡å‡†
        platform_issues = self._check_platform_standards(platform, title, author, generated_content)
        issues.extend(platform_issues)
        quality_score -= len(platform_issues) * 0.1
        
        # 3. æ£€æŸ¥å†…å®¹å”¯ä¸€æ€§
        if content_hash in [h for hashes in self.content_hashes.values() for h in hashes]:
            issues.append("å†…å®¹ä¸å…¶ä»–æ–‡æ¡£é‡å¤")
            quality_score -= 0.3
        
        # 4. è®¡ç®—å†…å®¹ä¸°å¯Œåº¦
        unique_keywords = len(set(generated_content.split())) if generated_content else 0
        if unique_keywords < 50:
            issues.append(f"å†…å®¹è¿‡äºç®€å• (å”¯ä¸€è¯æ±‡: {unique_keywords})")
            quality_score -= 0.2
        
        # 5. æ£€æŸ¥æ ‡é¢˜å’Œä½œè€…æœ‰æ•ˆæ€§
        if title in self.platform_standards.get(platform, {}).get('forbidden_titles', []):
            issues.append(f"æ ‡é¢˜æ— æ•ˆ: '{title}'")
            quality_score -= 0.3
        
        if author == 'æœªçŸ¥ä½œè€…':
            issues.append("ä½œè€…ä¿¡æ¯ç¼ºå¤±")
            quality_score -= 0.1
        
        quality_score = max(0.0, quality_score)
        
        metrics = DocumentQualityMetrics(
            url=url,
            title=title,
            content_length=len(generated_content),
            unique_keywords=unique_keywords,
            content_hash=content_hash,
            platform=platform,
            author=author,
            timestamp=time.time(),
            quality_score=quality_score,
            issues=issues
        )
        
        # è®°å½•åˆ°ç›‘ç£æ•°æ®
        self.document_metrics.append(metrics)
        self.content_hashes[content_hash].append(url)
        
        return metrics
    
    def _check_template_content(self, title: str, content: str) -> float:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé€šç”¨æ¨¡æ¿å†…å®¹"""
        template_indicators = 0
        total_indicators = len(self.template_keywords)
        
        content_lower = content.lower()
        title_lower = title.lower()
        
        for keyword in self.template_keywords:
            if keyword in content_lower or keyword in title_lower:
                template_indicators += 1
        
        return template_indicators / total_indicators if total_indicators > 0 else 0.0
    
    def _check_platform_standards(self, platform: str, title: str, author: str, content: str) -> List[str]:
        """æ£€æŸ¥å¹³å°ç‰¹å®šè´¨é‡æ ‡å‡†"""
        issues = []
        standards = self.platform_standards.get(platform, {})
        
        # æ£€æŸ¥æ ‡é¢˜é•¿åº¦
        min_title_length = standards.get('min_title_length', 3)
        if len(title) < min_title_length:
            issues.append(f"æ ‡é¢˜è¿‡çŸ­ (é•¿åº¦: {len(title)}, è¦æ±‚: >={min_title_length})")
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        min_content_length = standards.get('min_content_length', 100)
        if len(content) < min_content_length:
            issues.append(f"å†…å®¹è¿‡çŸ­ (é•¿åº¦: {len(content)}, è¦æ±‚: >={min_content_length})")
        
        # æ£€æŸ¥ç¦ç”¨æ ‡é¢˜
        forbidden_titles = standards.get('forbidden_titles', [])
        if title in forbidden_titles:
            issues.append(f"ä½¿ç”¨äº†ç¦ç”¨æ ‡é¢˜: '{title}'")
        
        return issues
    
    def analyze_batch_quality(self, batch_results: List[Dict[str, Any]]) -> BatchQualityReport:
        """
        åˆ†ææ‰¹é‡å¤„ç†çš„æ•´ä½“è´¨é‡
        
        å‚æ•°:
            batch_results: æ‰¹é‡å¤„ç†ç»“æœåˆ—è¡¨
            
        è¿”å›:
            æ‰¹é‡è´¨é‡æŠ¥å‘Š
        """
        batch_id = f"batch_{int(time.time())}"
        total_documents = len(batch_results)
        
        # ç»Ÿè®¡é‡å¤å’Œæ¨¡æ¿æ–‡æ¡£
        content_hashes = {}
        template_count = 0
        platform_stats = defaultdict(lambda: {"total": 0, "success": 0, "quality_issues": 0})
        quality_issues = []
        
        for result in batch_results:
            if result.get('status') == 'success':
                # è¿™é‡Œéœ€è¦å®é™…çš„æ–‡æ¡£å†…å®¹æ¥åˆ†æï¼Œæš‚æ—¶ç”¨æ¨¡æ‹Ÿæ•°æ®
                url = result.get('url', '')
                platform = self._detect_platform(url)
                title = result.get('title', 'æœªçŸ¥æ ‡é¢˜')
                
                platform_stats[platform]["total"] += 1
                platform_stats[platform]["success"] += 1
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨¡æ¿å†…å®¹
                if self._is_template_title(title):
                    template_count += 1
                    platform_stats[platform]["quality_issues"] += 1
                    quality_issues.append(f"æ£€æµ‹åˆ°æ¨¡æ¿æ–‡æ¡£: {url} (æ ‡é¢˜: {title})")
        
        # è®¡ç®—å”¯ä¸€æ–‡æ¡£æ•°
        unique_documents = total_documents - template_count
        duplicate_documents = total_documents - len(set(content_hashes.keys()))
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(platform_stats, quality_issues)
        
        report = BatchQualityReport(
            batch_id=batch_id,
            total_documents=total_documents,
            unique_documents=unique_documents,
            duplicate_documents=duplicate_documents,
            template_documents=template_count,
            quality_issues=quality_issues,
            platform_stats=dict(platform_stats),
            recommendations=recommendations,
            timestamp=time.time()
        )
        
        self.batch_history.append(report)
        
        # å¦‚æœè´¨é‡é—®é¢˜ä¸¥é‡ï¼Œè§¦å‘è­¦æŠ¥
        if template_count > total_documents * 0.5:  # è¶…è¿‡50%æ˜¯æ¨¡æ¿
            self._trigger_quality_alert(report)
        
        return report
    
    def _detect_platform(self, url: str) -> str:
        """æ£€æµ‹URLå¯¹åº”çš„å¹³å°"""
        url_lower = url.lower()
        if "douyin.com" in url_lower:
            return "douyin"
        elif "xiaohongshu.com" in url_lower:
            return "xiaohongshu"
        elif "zhihu.com" in url_lower:
            return "zhihu"
        elif "bilibili.com" in url_lower:
            return "bilibili"
        else:
            return "unknown"
    
    def _is_template_title(self, title: str) -> bool:
        """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦ä¸ºæ¨¡æ¿æ ‡é¢˜"""
        template_titles = ["è§†é¢‘", "æœªçŸ¥æ ‡é¢˜", "ç»¼åˆçŸ¥è¯†", "æ ¸å¿ƒæ¦‚å¿µä¸ç†è®º"]
        return title in template_titles or any(keyword in title for keyword in self.template_keywords)
    
    def _generate_recommendations(self, platform_stats: Dict, quality_issues: List[str]) -> List[str]:
        """ç”Ÿæˆè´¨é‡æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # æ£€æŸ¥å„å¹³å°è´¨é‡
        for platform, stats in platform_stats.items():
            if stats["quality_issues"] > stats["success"] * 0.3:  # è´¨é‡é—®é¢˜è¶…è¿‡30%
                recommendations.append(f"å¹³å° {platform} è´¨é‡é—®é¢˜è¾ƒå¤šï¼Œå»ºè®®æ£€æŸ¥æå–å™¨é…ç½®")
        
        # æ£€æŸ¥æ•´ä½“è´¨é‡é—®é¢˜
        if len(quality_issues) > 0:
            recommendations.append("æ£€æµ‹åˆ°æ¨¡æ¿æ–‡æ¡£ï¼Œå»ºè®®:")
            recommendations.append("1. éªŒè¯è§†é¢‘é“¾æ¥çš„æœ‰æ•ˆæ€§")
            recommendations.append("2. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œè®¿é—®æƒé™")
            recommendations.append("3. ä¸ºéš¾ä»¥æå–çš„é“¾æ¥æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯")
            recommendations.append("4. è€ƒè™‘æ›´æ–°æå–å™¨ç®—æ³•")
        
        return recommendations
    
    def _trigger_quality_alert(self, report: BatchQualityReport) -> None:
        """è§¦å‘è´¨é‡è­¦æŠ¥"""
        logger.warning("ğŸš¨ è´¨é‡ç›‘ç£è­¦æŠ¥!")
        logger.warning(f"æ‰¹æ¬¡ {report.batch_id} æ£€æµ‹åˆ°ä¸¥é‡è´¨é‡é—®é¢˜:")
        logger.warning(f"- æ€»æ–‡æ¡£æ•°: {report.total_documents}")
        logger.warning(f"- æ¨¡æ¿æ–‡æ¡£æ•°: {report.template_documents}")
        logger.warning(f"- è´¨é‡é—®é¢˜: {len(report.quality_issues)}")
        
        for issue in report.quality_issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
            logger.warning(f"  â€¢ {issue}")
        
        if self.enable_llm_assistance:
            self._request_llm_assistance(report)
    
    def _request_llm_assistance(self, report: BatchQualityReport) -> None:
        """è¯·æ±‚LLMè¾…åŠ©è¯Šæ–­"""
        logger.info("ğŸ¤– è¯·æ±‚LLMè¾…åŠ©è¯Šæ–­...")
        
        # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„LLM APIè°ƒç”¨
        # æš‚æ—¶è®°å½•è¯Šæ–­è¯·æ±‚
        diagnosis_request = {
            "timestamp": time.time(),
            "batch_id": report.batch_id,
            "issues": report.quality_issues,
            "platform_stats": report.platform_stats,
            "request_type": "quality_diagnosis"
        }
        
        logger.info("LLMè¯Šæ–­è¯·æ±‚å·²è®°å½•ï¼Œå»ºè®®äººå·¥å®¡æŸ¥æ‰¹é‡å¤„ç†ç»“æœ")
    
    def get_quality_summary(self) -> Dict[str, Any]:
        """è·å–è´¨é‡ç›‘ç£æ€»ç»“"""
        if not self.document_metrics:
            return {"status": "no_data", "message": "æš‚æ— ç›‘ç£æ•°æ®"}
        
        total_docs = len(self.document_metrics)
        high_quality_docs = sum(1 for m in self.document_metrics if m.quality_score >= self.min_quality_score)
        
        # ç»Ÿè®¡å„å¹³å°è´¨é‡
        platform_quality = defaultdict(list)
        for metric in self.document_metrics:
            platform_quality[metric.platform].append(metric.quality_score)
        
        platform_avg_quality = {
            platform: sum(scores) / len(scores) if scores else 0.0
            for platform, scores in platform_quality.items()
        }
        
        # ç»Ÿè®¡å¸¸è§é—®é¢˜
        all_issues = []
        for metric in self.document_metrics:
            all_issues.extend(metric.issues)
        
        issue_counts = defaultdict(int)
        for issue in all_issues:
            issue_counts[issue] += 1
        
        return {
            "total_documents": total_docs,
            "high_quality_documents": high_quality_docs,
            "quality_rate": high_quality_docs / total_docs if total_docs > 0 else 0.0,
            "platform_quality": platform_avg_quality,
            "common_issues": dict(sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)[:5]),
            "duplicate_content_groups": len([h for h, urls in self.content_hashes.items() if len(urls) > 1]),
            "recent_batches": len(self.batch_history)
        }
    
    def save_report(self, output_dir: str = "temp/quality_reports") -> str:
        """ä¿å­˜è´¨é‡ç›‘ç£æŠ¥å‘Š"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = int(time.time())
        report_file = output_path / f"quality_report_{timestamp}.json"
        
        report_data = {
            "summary": self.get_quality_summary(),
            "document_metrics": [m.to_dict() for m in self.document_metrics],
            "batch_history": [b.to_dict() for b in self.batch_history],
            "generated_at": time.time()
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è´¨é‡ç›‘ç£æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return str(report_file)

# å…¨å±€è´¨é‡ç›‘ç£å™¨å®ä¾‹
quality_supervisor = QualitySupervisor() 